---
title: Reducing Data Loading Bottleneck with Coarse Feature Vectors for Large Scale
  Learning
abstract: In large scale learning, disk I/O for data loading is often the runtime
  bottleneck. We propose a lossy data compression scheme with a fast decompression
  to reduce disk I/O, allocating fewer than the standard 32 bits for each real value
  in the data set. We theoretically show that the estimation error induced by the
  loss in compression decreases exponentially with the number of the bits used per
  value. Our experiments show the proposed method achieves excellent performance with
  a small number of bits and substantial speedups during training.
layout: inproceedings
id: takamatsu14
month: 0
firstpage: 46
lastpage: 60
page: 46-60
sections: 
author:
- given: Shingo
  family: Takamatsu
- given: Carlos
  family: Guestrin
date: 2014-08-13
address: New York, New York, USA
publisher: PMLR
container-title: 'Proceedings of the 3rd International Workshop on Big Data, Streams
  and Heterogeneous Source Mining: Algorithms, Systems, Programming Models and Applications'
volume: '36'
genre: inproceedings
issued:
  date-parts:
  - 2014
  - 8
  - 13
pdf: http://proceedings.mlr.press/v36/takamatsu14/takamatsu14.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
