<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
	<title>Reducing Data Loading Bottleneck with Coarse Feature Vectors for Large Scale Learning | BIGMINE 2014 | JMLR W&amp;CP</title>

	<!-- Stylesheet -->
	<link rel="stylesheet" type="text/css" href="../css/jmlr.css" />

	<!-- Fixed position navigation -->
	<!--#include virtual="/proceedings/css-scroll.txt"-->

	<!-- MathJax -->
	<script type="text/x-mathjax-config">
	MathJax.Hub.Config({tex2jax: {inlineMath: [['\\(','\\)']]}});
</script>
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


	<!-- Metadata -->
	<!-- Google Scholar Meta Data -->

<meta name="citation_title" content="Reducing Data Loading Bottleneck with Coarse Feature Vectors for Large Scale Learning">

  <meta name="citation_author" content="Takamatsu, Shingo">

  <meta name="citation_author" content="Guestrin, Carlos">

<meta name="citation_publication_date" content="2014">
<meta name="citation_conference_title" content="Proceedings of the 3rd International Workshop on Big Data, Streams and Heterogeneous Source Mining: Algorithms, Systems, Programming Models and Applications">
<meta name="citation_firstpage" content="46">
<meta name="citation_lastpage" content="60">
<meta name="citation_pdf_url" content="takamatsu14.pdf">

</head>
<body>


<div id="fixed">
<!--#include virtual="/proceedings/nav-bar.txt"-->
</div>

<div id="content">

	<h1>Reducing Data Loading Bottleneck with Coarse Feature Vectors for Large Scale Learning</h1>

	<div id="authors">
	
		Shingo Takamatsu,
	
		Carlos Guestrin
	</div>;
	<div id="info">
		JMLR W&amp;CP 36 
		
		: 
		46â€“60, 2014
	</div> <!-- info -->

	

	<h2>Abstract</h2>
	<div id="abstract">
		In large scale learning, disk I/O for data loading is often the runtime bottleneck. We propose a lossy data compression scheme with a fast decompression to reduce disk I/O, allocating fewer than the standard 32 bits for each real value in the data set. We theoretically show that the estimation error induced by the loss in compression decreases exponentially with the number of the bits used per value. Our experiments show the proposed method achieves excellent performance with a small number of bits and substantial speedups during training.
	</div>

	<h2>Related Material</h2>
	<div id="extras">
		<ul>
			<li><a href="takamatsu14.pdf">Download PDF</a></li>
			
			
		</ul>
	</div> <!-- extras -->

</div> <!-- content -->

</body>
</html>
